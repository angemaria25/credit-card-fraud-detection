{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_score\n",
    "from preprocess import preprocess_fraud_data\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_auc_score, f1_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"./data/dataset_balanceado.csv\"\n",
    "df = pd.read_csv(ruta)\n",
    "\n",
    "data = preprocess_fraud_data(df)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Base de Regresión Logística** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 90.91%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909091    0.0  0.909091     0.454545      0.826446\n",
      "recall        1.000000    0.0  0.909091     0.500000      0.909091\n",
      "f1-score      0.952381    0.0  0.909091     0.476190      0.865801\n",
      "support    6000.000000  600.0  0.909091  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[6000    0]\n",
      " [ 600    0]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 90.91%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909091    0.0  0.909091     0.454545      0.826446\n",
      "recall        1.000000    0.0  0.909091     0.500000      0.909091\n",
      "f1-score      0.952381    0.0  0.909091     0.476190      0.865801\n",
      "support    2000.000000  200.0  0.909091  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[2000    0]\n",
      " [ 200    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo_base = LogisticRegression(solver='liblinear', random_state=42)\n",
    "modelo_base.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo_base, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo_base, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - El modelo no es capaz de identificar las transacciones fraudulentas, al tener tan pocos datos de la clase minoritaria `Fraude`, el modelo simplemente predice todas las transacciones como legítimas, debido a  que la mayor cantidad de muestras en los datos que tiene para aprender son de transacciones legítimas, por lo cual decide clasificar el fraude como no fraude. \n",
    "> - El problema de desbalanceo de clases es tan notable que es necesario aplicar algunos métodos para tratar con clases desbalanceadas, además de que la métrica `accuracy` en estos casos es una métrica engañosa, pues devuelve **90.91%**, lo que podría parecer un resultado bastante bueno cuando en realidad el modelo no predice bien, porque lo que realmente importa es predecir correctamente las transacciones fraudulentas, no las no fraudulentas.\n",
    "> - En lo adelante hay que prestar más atención a las métricas que tienen más en cuenta a los datos de la clase minoritaria como `F1-score`, `Recall`, `precisión`, `matriz de confusión` , `AUC-ROC`, `PR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Técnicas de Muestreo:**\n",
    "\n",
    "- **Oversampling** (Sobremuestreo): Consiste en añadir copias de la clase minoritaria para aumentar su peso total.\n",
    "\n",
    "- **Undersampling** (Submuestreo): Se basa en eliminar muestras de la clase mayoritaria para intentar equilibrar el número de mustras en cada clase.\n",
    "\n",
    "- **Generación de muestras sintéticas**: En este caso se utilizan algoritmos como `SMOTE` que es capaz de generar más muestras de la clase minoritaria a partir de las que ya se tienen.\n",
    "\n",
    "- **Uso de pesos en la función de pérdida**: Una estrategia efectiva al entrenar modelos es modificar la función de pérdida asignando pesos mayores a la clase minoritaria, esto incentiva al modelo a prestar más atención a los ejemplos menos representados.\n",
    "\n",
    "- También se puede utilizar un enfoque híbrido que combina undersampling con oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original: [6000  600]\n",
      "Distribución después de undersampling: [600 600]\n",
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 55.00%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy    macro avg  weighted avg\n",
      "precision    0.549834    0.550167      0.55     0.550001      0.550001\n",
      "recall       0.551667    0.548333      0.55     0.550000      0.550000\n",
      "f1-score     0.550749    0.549249      0.55     0.549999      0.549999\n",
      "support    600.000000  600.000000      0.55  1200.000000   1200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[331 269]\n",
      " [271 329]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 48.59%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.917387    0.098361  0.485909     0.507874      0.842930\n",
      "recall        0.477500    0.570000  0.485909     0.523750      0.485909\n",
      "f1-score      0.628083    0.167770  0.485909     0.397927      0.586236\n",
      "support    2000.000000  200.000000  0.485909  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[ 955 1045]\n",
      " [  86  114]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribución original:\", np.bincount(y_train))\n",
    "\n",
    "#Undersampling Aleatorio\n",
    "undersampler = RandomUnderSampler(sampling_strategy=1, random_state=42)  # 1:1 ratio\n",
    "X_under, y_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Distribución después de undersampling:\", np.bincount(y_under))\n",
    "\n",
    "model_under = LogisticRegression(solver='liblinear', random_state=42)\n",
    "model_under.fit(X_under, y_under)\n",
    "\n",
    "print_score(model_under, X_under, y_under, X_val, y_val, train=True)\n",
    "print_score(model_under, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Al reducir el número de muestras de la clase mayoritaria el modelo es capaz de detectar el fraude, aunque sigue clasificando muchos fraudes como no fraude, además clasifica mal demasiados falsos positivos, está detectando la mayoria como si fuese fraude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original: [6000  600]\n",
      "Distribución después de undersampling: [6000 6000]\n",
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 53.37%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0            1  accuracy     macro avg  weighted avg\n",
      "precision     0.533937     0.533565   0.53375      0.533751      0.533751\n",
      "recall        0.531000     0.536500   0.53375      0.533750      0.533750\n",
      "f1-score      0.532464     0.535029   0.53375      0.533746      0.533746\n",
      "support    6000.000000  6000.000000   0.53375  12000.000000  12000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[3186 2814]\n",
      " [2781 3219]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 50.32%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.907457    0.089236  0.503182     0.498347      0.833074\n",
      "recall        0.505000    0.485000  0.503182     0.495000      0.503182\n",
      "f1-score      0.648892    0.150738  0.503182     0.399815      0.603605\n",
      "support    2000.000000  200.000000  0.503182  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1010  990]\n",
      " [ 103   97]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribución original:\", np.bincount(y_train))\n",
    "\n",
    "#Oversampling Aleatorio\n",
    "oversampler = RandomOverSampler(sampling_strategy=1, random_state=42)\n",
    "X_over, y_over = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Distribución después de undersampling:\", np.bincount(y_over))\n",
    "\n",
    "model_over = LogisticRegression(solver='liblinear', random_state=42)\n",
    "model_over.fit(X_over, y_over)\n",
    "\n",
    "print_score(model_over, X_over, y_over, X_val, y_val, train=True)\n",
    "print_score(model_over, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 63.04%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0            1  accuracy     macro avg  weighted avg\n",
      "precision     0.616983     0.647336  0.630417      0.632159      0.632159\n",
      "recall        0.687833     0.573000  0.630417      0.630417      0.630417\n",
      "f1-score      0.650485     0.607904  0.630417      0.629194      0.629194\n",
      "support    6000.000000  6000.000000  0.630417  12000.000000  12000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[4127 1873]\n",
      " [2562 3438]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 64.14%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.908294    0.089261  0.641364     0.498777      0.833836\n",
      "recall        0.673500    0.320000  0.641364     0.496750      0.641364\n",
      "f1-score      0.773471    0.139586  0.641364     0.456528      0.715845\n",
      "support    2000.000000  200.000000  0.641364  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1347  653]\n",
      " [ 136   64]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SMOTE (Generar muestras sintéticas)\n",
    "smote = SMOTE(sampling_strategy=1, random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entrenar modelo\n",
    "model_smote = LogisticRegression(solver='liblinear', random_state=42)\n",
    "model_smote.fit(X_smote, y_smote)\n",
    "\n",
    "print_score(model_smote, X_smote, y_smote, X_val, y_val, train=True)\n",
    "print_score(model_smote, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 52.14%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.918409    0.100780  0.521364     0.509595      0.844079\n",
      "recall        0.519667    0.538333  0.521364     0.529000      0.521364\n",
      "f1-score      0.663757    0.169777  0.521364     0.416767      0.618850\n",
      "support    6000.000000  600.000000  0.521364  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[3118 2882]\n",
      " [ 277  323]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 50.50%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.909991    0.091827     0.505     0.500909      0.835612\n",
      "recall        0.505500    0.500000     0.505     0.502750      0.505000\n",
      "f1-score      0.649952    0.155159     0.505     0.402555      0.604971\n",
      "support    2000.000000  200.000000     0.505  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1011  989]\n",
      " [ 100  100]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_balanced = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    class_weight='balanced',  # Ajusta pesos automáticamente\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_balanced.fit(X_train, y_train) \n",
    "\n",
    "print_score(model_balanced, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(model_balanced, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTETomek:** Técnica híbrida que combina oversampling (SMOTE) con undersampling (Tomek Links).\n",
    "\n",
    "El proceso es el siguiente:\n",
    "\n",
    "- **Oversampling** (SMOTE): Primero, se aplica SMOTE al conjunto de datos de entrenamiento, esto crea nuevas muestras sintéticas de la clase minoritaria (fraude) hasta alcanzar un cierto balance (generalmente 1:1, pero se puede configurar).\n",
    "\n",
    "- **Undersampling** (Tomek Links): Después de generar los nuevos puntos de fraude, se aplica Tomek Links, un \"Tomek Link\" es un par de puntos de clases opuestas que son los vecinos más cercanos el uno del otro, en este paso, se eliminan las muestras de la clase mayoritaria que forman parte de un Tomek Link.\n",
    "\n",
    "SMOTE puede generar puntos sintéticos en regiones `ruidosas`, muy cerca de la frontera de decisión o incluso dentro del territorio de la clase mayoritaria. Al aplicar Tomek Links después, se `limpia` la frontera de decisión, eliminando puntos de la clase mayoritaria que están demasiado cerca de los puntos de la clase minoritaria (tanto los originales como los sintéticos). Esto ayuda a que el modelo aprenda una separación entre clases más clara y a menudo mejora la precisión sin sacrificar mucho el recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original en entrenamiento: [6000  600]\n",
      "Distribución después de SMOTETomek: [5982 5982]\n",
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 63.08%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0            1  accuracy     macro avg  weighted avg\n",
      "precision     0.617158     0.648061  0.630809      0.632609      0.632609\n",
      "recall        0.689067     0.572551  0.630809      0.630809      0.630809\n",
      "f1-score      0.651133     0.607970  0.630809      0.629552      0.629552\n",
      "support    5982.000000  5982.000000  0.630809  11964.000000  11964.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[4122 1860]\n",
      " [2557 3425]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 64.14%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.908294    0.089261  0.641364     0.498777      0.833836\n",
      "recall        0.673500    0.320000  0.641364     0.496750      0.641364\n",
      "f1-score      0.773471    0.139586  0.641364     0.456528      0.715845\n",
      "support    2000.000000  200.000000  0.641364  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1347  653]\n",
      " [ 136   64]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# 1. Crear el objeto SMOTETomek\n",
    "# Por defecto, SMOTE intentará balancear a 1:1\n",
    "smt_tomek = SMOTETomek(random_state=42)\n",
    "\n",
    "#Aplicar el remuestreo SOLO al conjunto de entrenamiento\n",
    "print(\"Distribución original en entrenamiento:\", np.bincount(y_train))\n",
    "X_train_resampled, y_train_resampled = smt_tomek.fit_resample(X_train, y_train)\n",
    "print(\"Distribución después de SMOTETomek:\", np.bincount(y_train_resampled))\n",
    "\n",
    "model_smotetomek = LogisticRegression(solver='liblinear', random_state=42)\n",
    "model_smotetomek.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print_score(model_smotetomek, X_train_resampled, y_train_resampled, X_val, y_val, train=True)\n",
    "print_score(model_smotetomek, X_train, y_train, X_val, y_val, train=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
