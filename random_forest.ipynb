{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_score\n",
    "from preprocess import preprocess_fraud_data\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"./data/dataset_balanceado.csv\"\n",
    "df = pd.read_csv(ruta)\n",
    "\n",
    "data = preprocess_fraud_data(df)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Base de Random Forest** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 99.98%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.999833    1.000000  0.999848     0.999917      0.999849\n",
      "recall        1.000000    0.998333  0.999848     0.999167      0.999848\n",
      "f1-score      0.999917    0.999166  0.999848     0.999541      0.999848\n",
      "support    6000.000000  600.000000  0.999848  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[6000    0]\n",
      " [   1  599]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 90.86%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909050    0.0  0.908636     0.454525      0.826409\n",
      "recall        0.999500    0.0  0.908636     0.499750      0.908636\n",
      "f1-score      0.952131    0.0  0.908636     0.476066      0.865574\n",
      "support    2000.000000  200.0  0.908636  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1999    1]\n",
      " [ 200    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo = RandomForestClassifier(random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> El modelo base Random Forest con sus parámetros por defecto, sufre de `sobreajuste` (overfitting), lo que significa que ha vuelto un experto en memorizar los datos de entrenamiento, pero no es capaz de detectar ningún fraude en datos que nunca ha visto, lo cual lo convierte en un modelo inútil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 100 árboles.\n",
      "------------------------------\n",
      "Profundidad máxima encontrada: 33\n",
      "Profundidad mínima encontrada: 21\n",
      "Profundidad promedio: 25.85\n",
      "\n",
      "Profundidades de los primeros 10 árboles:\n",
      "[22, 23, 22, 22, 24, 25, 23, 23, 27, 24]\n"
     ]
    }
   ],
   "source": [
    "# 'modelo.estimators_' es una lista de todos los árboles de decisión entrenados\n",
    "profundidades = [arbol.get_depth() for arbol in modelo.estimators_]\n",
    "\n",
    "print(f\"El modelo tiene {len(profundidades)} árboles.\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Profundidad máxima encontrada: {max(profundidades)}\")\n",
    "print(f\"Profundidad mínima encontrada: {min(profundidades)}\")\n",
    "print(f\"Profundidad promedio: {np.mean(profundidades):.2f}\")\n",
    "\n",
    "print(\"\\nProfundidades de los primeros 10 árboles:\")\n",
    "print(profundidades[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> El modelo cuenta con 100 árboles, esto árboles tienen una profundidad máxima de 33, la mínima de 21 y el promedio es de casi 26 niveles.\n",
    ">\n",
    "> Al no ponerle límites (max_depth=None) a los árboles, se permitió que cada árbol creciera sin control, lo que los llevó a seguir bajando y hacer preguntas muy específicas permitiendoles ser demasiados detallados, lo cual les permite aprende hasta el mínimo detalle y memorizarlo, por eso es que el modelo funciona tan bien en entrenamiento pero es pésimo en validación, porque son datos que nunca a visto, el modelo solo memoriza no sabe generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podar árboles:** Limitar la profundidad y que cada hoja tenga un número mínimo de ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 90.91%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909091    0.0  0.909091     0.454545      0.826446\n",
      "recall        1.000000    0.0  0.909091     0.500000      0.909091\n",
      "f1-score      0.952381    0.0  0.909091     0.476190      0.865801\n",
      "support    6000.000000  600.0  0.909091  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[6000    0]\n",
      " [ 600    0]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 90.91%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909091    0.0  0.909091     0.454545      0.826446\n",
      "recall        1.000000    0.0  0.909091     0.500000      0.909091\n",
      "f1-score      0.952381    0.0  0.909091     0.476190      0.865801\n",
      "support    2000.000000  200.0  0.909091  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[2000    0]\n",
      " [ 200    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo_mejorado = RandomForestClassifier(max_depth=10, min_samples_leaf=10, min_samples_split=10, random_state=42)\n",
    "modelo_mejorado.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo_mejorado, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo_mejorado, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Al podar los árboles con max_depth, min_samples_leaf y min_samples_split, se ha resuelto el problema del sobreajuste, sin embargo, al hacerlo, se ha vuelto al problema inicial de nuestros datos, el desbalance de clases. El modelo actual, aunque ya no memoriza, sigue siendo incapaz de identificar el fraude, lo que lo mantiene siendo un modelo inútil.\n",
    ">\n",
    "> Ahora que el modelo no puede memorizar casos específicos (debido a la poda), se ve forzado a aprender reglas generales. Dado que la clase fraude, es la clase minoritaria, la estrategia por la que opta el modelo es ignorar por completo a la clase con menor cantidda e muestras, el algoritmo concluye que el costo de equivocarse al intentar predecir un fraude es mayor que el beneficio de acertar, por lo que adopta la política de siempre predecir la clase mayoritaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class_weight='balanced'**\n",
    "\n",
    "Ajuste de pesos de clase., esta técnica funcionará como un multiplicador de penalización.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 94.68%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.970672    0.707846  0.946818     0.839259      0.946778\n",
      "recall        0.970833    0.706667  0.946818     0.838750      0.946818\n",
      "f1-score      0.970752    0.707256  0.946818     0.839004      0.946798\n",
      "support    6000.000000  600.000000  0.946818  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[5825  175]\n",
      " [ 176  424]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 86.32%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.908217    0.075630  0.863182     0.491924      0.832527\n",
      "recall        0.945000    0.045000  0.863182     0.495000      0.863182\n",
      "f1-score      0.926244    0.056426  0.863182     0.491335      0.847169\n",
      "support    2000.000000  200.000000  0.863182  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1890  110]\n",
      " [ 191    9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo_balanceado = RandomForestClassifier(max_depth=10, min_samples_leaf=10, min_samples_split=10, class_weight='balanced', random_state=42)\n",
    "modelo_balanceado.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo_balanceado, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo_balanceado, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Al combinar la regularización para controlar el sobreajuste con el parámetro class_weight='balanced' para atacar el desbalanceo, el modelo ha aprendido a identificar transacciones fraudulentas, se ha pasado de un modelo que ignoraba por completo el fraude (Recall de 0.0) a uno que lo detecta, sin embargo, los resultados actuales generan muchas falsas alarmas, y todavía no es capaz de identificar correctamente todos los verdaderos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 84.56%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.927114    0.227568  0.845606     0.577341      0.863519\n",
      "recall        0.901000    0.291667  0.845606     0.596333      0.845606\n",
      "f1-score      0.913870    0.255661  0.845606     0.584766      0.854033\n",
      "support    6000.000000  600.000000  0.845606  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[5406  594]\n",
      " [ 425  175]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 81.41%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.907322    0.076923  0.814091     0.492123      0.831831\n",
      "recall        0.886000    0.095000  0.814091     0.490500      0.814091\n",
      "f1-score      0.896534    0.085011  0.814091     0.490773      0.822759\n",
      "support    2000.000000  200.000000  0.814091  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1772  228]\n",
      " [ 181   19]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Pipeline que primero aplica SMOTE y luego entrena el clasificador regularizado\n",
    "pipeline_smote = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(max_depth=10, min_samples_leaf=10, random_state=42))\n",
    "])\n",
    "pipeline_smote.fit(X_train, y_train)\n",
    "\n",
    "print_score(pipeline_smote, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(pipeline_smote, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE\n",
    "\n",
    "Balancear el set de entrenamiento creando nuevos ejemplos sintéticos de fraude lo cual le dará al modelo muchos más ejemplos de los que aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 85.51%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0             1  accuracy      macro avg   weighted avg\n",
      "precision      0.846829      0.863671  0.855051       0.855250       0.855250\n",
      "recall         0.866902      0.843199  0.855051       0.855051       0.855051\n",
      "f1-score       0.856748      0.853312  0.855051       0.855030       0.855030\n",
      "support    59400.000000  59400.000000  0.855051  118800.000000  118800.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[51494  7906]\n",
      " [ 9314 50086]]\n",
      "\n",
      "\n",
      "--- Resultados del Modelo con SMOTE en el set de Validación ---\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 85.95%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.989964    0.009763   0.85945      0.499864      0.980162\n",
      "recall         0.866818    0.130000   0.85945      0.498409      0.859450\n",
      "f1-score       0.924307    0.018163   0.85945      0.471235      0.915246\n",
      "support    19800.000000  200.000000   0.85945  20000.000000  20000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[17163  2637]\n",
      " [  174    26]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "modelo_final = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=5, min_samples_split=10, random_state=42)\n",
    "\n",
    "modelo_final.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print_score(modelo_final, X_train_resampled, y_train_resampled, X_val, y_val, train=True)\n",
    "print_score(modelo_final, X_train_resampled, y_train_resampled, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajustar hiperpárametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando GridSearchCV... Probando 54 combinaciones.\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   9.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   9.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  10.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  10.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  12.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  13.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  18.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  15.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=   9.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=   9.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=   9.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=   8.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=   8.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=   8.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=   8.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=   8.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=   8.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  13.3s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  11.8s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  11.7s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  12.0s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  12.0s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  11.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  11.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  11.3s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  12.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  11.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  12.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  16.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  17.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  15.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  14.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  17.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  14.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  14.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  17.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  22.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  15.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  17.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  16.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  16.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  15.6s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  16.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  15.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  15.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  16.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  17.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  17.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  12.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  15.7s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   9.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  12.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  14.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  10.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  19.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  11.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  11.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  17.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  13.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  21.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  15.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  12.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  12.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  14.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=  14.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=  12.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=  15.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=  18.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=  16.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  26.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  21.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  15.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  15.1s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  14.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  15.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  15.2s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  15.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  14.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  15.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  14.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  15.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  16.3s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  15.6s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  12.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  12.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  12.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  12.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  12.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  14.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  13.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  14.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  14.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  13.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  14.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  13.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  15.6s\n",
      "\n",
      "--- Resultados de GridSearchCV ---\n",
      "Mejor puntuación (f1_macro): 0.9922\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\n",
      "--- Resultados del Modelo OPTIMIZADO en el set de Validación ---\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 98.41%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0      1  accuracy     macro avg  weighted avg\n",
      "precision      0.989941    0.0    0.9841      0.494970      0.980041\n",
      "recall         0.994040    0.0    0.9841      0.497020      0.984100\n",
      "f1-score       0.991986    0.0    0.9841      0.495993      0.982066\n",
      "support    19800.000000  200.0    0.9841  20000.000000  20000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[19682   118]\n",
      " [  200     0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, None],          # Profundidad del árbol. None = sin límite.\n",
    "    'min_samples_leaf': [2, 5, 10],       # Mínimo de muestras en una hoja.\n",
    "    'min_samples_split': [5, 10, 20],     # Mínimo para dividir un nodo.\n",
    "    'criterion': ['gini', 'entropy']      # Criterio para medir la calidad de una división.\n",
    "}\n",
    "\n",
    "# métrica de evaluación\n",
    "scoring_metric =  'f1_macro'\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "# cv=3 significa 3-fold cross-validation. Para cada una de las 54 combinaciones,\n",
    "# entrenará y validará el modelo 3 veces. Total de entrenamientos: 54 * 3 = 162.\n",
    "grid_search = GridSearchCV(estimator=rf, \n",
    "                            param_grid=param_grid, \n",
    "                            cv=3, \n",
    "                            scoring=scoring_metric,\n",
    "                            verbose=2) # verbose=2 te mostrará el progreso\n",
    "\n",
    "print(f\"\\nIniciando GridSearchCV... Probando {len(param_grid['max_depth'])*len(param_grid['min_samples_leaf'])*len(param_grid['min_samples_split'])*len(param_grid['criterion'])} combinaciones.\")\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"\\n--- Resultados de GridSearchCV ---\")\n",
    "print(f\"Mejor puntuación ({scoring_metric}): {grid_search.best_score_:.4f}\")\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\n--- Resultados del Modelo OPTIMIZADO en el set de Validación ---\")\n",
    "print_score(best_rf_model, X_train_resampled, y_train_resampled, X_val, y_val, train=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
