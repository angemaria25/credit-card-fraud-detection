{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import cargar_y_preprocesar_datos\n",
    "from utils import print_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"./data/credit_card_fraud_dataset.csv\"\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = cargar_y_preprocesar_datos(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 99.99%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.999933    1.000000  0.999933      0.999966      0.999933\n",
      "recall         1.000000    0.993333  0.999933      0.996667      0.999933\n",
      "f1-score       0.999966    0.996656  0.999933      0.998311      0.999933\n",
      "support    59400.000000  600.000000  0.999933  60000.000000  60000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[59400     0]\n",
      " [    4   596]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 99.00%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0      1  accuracy     macro avg  weighted avg\n",
      "precision      0.989999    0.0   0.98995      0.495000       0.98010\n",
      "recall         0.999949    0.0   0.98995      0.499975       0.98995\n",
      "f1-score       0.994950    0.0   0.98995      0.497475       0.98500\n",
      "support    19800.000000  200.0   0.98995  20000.000000   20000.00000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[19799     1]\n",
      " [  200     0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo = RandomForestClassifier(random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "print_score(modelo, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene un 99.99%  de accuracy en el entrenamiento y clasifica correctanmente todos los verdaderos negativos y solo se equivoca en 4 transacciones fraudulentas pero en validación funciona mal, no clasifica ningún verdadero positivo, lo que quiere decir que mi modelo esta memorizando los datos de entrenamiento, es decir, tiene sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene una exactitud global muy alta, pero hay que tener en cuenta que se debe sobre todo a que las transacciones normales tienen mucha má frecuencia que las fraudulentas y tenemos un dataset desbalanceado en favor de la clase 0 que es la mayoritaria. Hay que tener en cuenta que en el contexto de detección de fraude lo que nos interesa es detectar aquellas transacciones que son fraudulentas pero que no han sido detectadas, es decir, los falsos negativos para la clase 1, que como hemos visto en la correspondiente métrica de recall supone un 26% del total de transacciones categorizadas como fraudulentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 99.00%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0      1  accuracy     macro avg  weighted avg\n",
      "precision      0.990000    0.0      0.99      0.495000      0.980100\n",
      "recall         1.000000    0.0      0.99      0.500000      0.990000\n",
      "f1-score       0.994975    0.0      0.99      0.497487      0.985025\n",
      "support    59400.000000  600.0      0.99  60000.000000  60000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[59400     0]\n",
      " [  600     0]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 99.00%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0      1  accuracy     macro avg  weighted avg\n",
      "precision      0.990000    0.0      0.99      0.495000      0.980100\n",
      "recall         1.000000    0.0      0.99      0.500000      0.990000\n",
      "f1-score       0.994975    0.0      0.99      0.497487      0.985025\n",
      "support    19800.000000  200.0      0.99  20000.000000  20000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[19800     0]\n",
      " [  200     0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "modelo_mejorado = RandomForestClassifier(n_estimators=100,  max_depth=10, min_samples_leaf=5, min_samples_split=10, random_state=42)\n",
    "modelo_mejorado.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo_mejorado, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo_mejorado, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este nuevo modelo se ha solucionado el sobreajuste, antes el El rendimiento en entrenamiento era 99.99% y en validación 99.00%, ahora el  rendimiento en entrenamiento es 99.00% y en validación también es 99.00%, el modelo ya no está memorizando, ha aprendido un patrón general y lo aplica tanto a los datos que ha visto como a los que no.\n",
    "\n",
    "Si se observa la matriz de confusión tanto en entrenamiento como en validación, el modelo ha aprendido que la mejor estrategia para maximizar la accuracy es ignorar por completo la clase 1, no ha sido capaz de encontrar un patrón  en los 600 ejemplos de fraude como para justificar el riesgo de una predicción incorrecta.\n",
    "\n",
    "La regularización por sí sola no es suficiente porque hay un gran desbalance en las clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicar sobremuestreo con SMOTE**\n",
    "\n",
    "Esta técnica `balanceará` el set de entrenamiento creando nuevos ejemplos sintéticos de fraude, lo cual le dará al modelo muchos más ejemplos de los que aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 85.51%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0             1  accuracy      macro avg   weighted avg\n",
      "precision      0.846829      0.863671  0.855051       0.855250       0.855250\n",
      "recall         0.866902      0.843199  0.855051       0.855051       0.855051\n",
      "f1-score       0.856748      0.853312  0.855051       0.855030       0.855030\n",
      "support    59400.000000  59400.000000  0.855051  118800.000000  118800.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[51494  7906]\n",
      " [ 9314 50086]]\n",
      "\n",
      "\n",
      "--- Resultados del Modelo con SMOTE en el set de Validación ---\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 85.95%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0           1  accuracy     macro avg  weighted avg\n",
      "precision      0.989964    0.009763   0.85945      0.499864      0.980162\n",
      "recall         0.866818    0.130000   0.85945      0.498409      0.859450\n",
      "f1-score       0.924307    0.018163   0.85945      0.471235      0.915246\n",
      "support    19800.000000  200.000000   0.85945  20000.000000  20000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[17163  2637]\n",
      " [  174    26]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Aplicar SMOTE SOLO al conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#Usar modelo regularizado \n",
    "#Ya sabemos que este modelo no sobreajusta, así que es perfecto para usarlo con los nuevos datos\n",
    "modelo_final = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=5, min_samples_split=10, random_state=42)\n",
    "\n",
    "#Entrenar con los datos REMUESTREADOS\n",
    "modelo_final.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "#Evaluar en los datos de validación ORIGINALES (desbalanceados)\n",
    "print_score(modelo_final, X_train_resampled, y_train_resampled, X_val, y_val, train=True)\n",
    "print_score(modelo_final, X_train_resampled, y_train_resampled, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: aumentar la Precisión sin sacrificar demasiado el Recall, se quieren menos falsas alarmas.\n",
    "\n",
    "- Ajuste de hiperparámetros: usar `GridSearchCV` para encontrar una mejor combinación.\n",
    "    - Buscar un balance: aumentar max_depth un poco (ej. 12, 15) para que el modelo pueda aprender reglas más complejas y ser más preciso.\n",
    "    - Aumentar min_samples_leaf (ej. 10, 20) para que el modelo sea más conservador y genere menos falsos positivos.\n",
    "\n",
    "- Ajustar el Umbral de Decisión:\n",
    "    - Por defecto `predict()` usa un umbral de 0.5, si la probabilidad de fraude es > 0.5, lo clasifica como fraude.\n",
    "    - Dado que se tienen muchos `Falsos Positivos`, significa que muchas transacciones legítimas están obteniendo una puntuación de fraude de, por ej. 0.55 o 0.6.\n",
    "    - Subir el umbral: En lugar de 0.5, prueba a clasificar como fraude solo si la probabilidad es > 0.8 o 0.9. Esto reducirá drásticamente los Falsos Positivos (aumentando la Precisión), pero probablemente también reducirá un poco los Verdaderos Positivos (bajando el Recall). Tienes que encontrar el punto dulce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando GridSearchCV... Probando 54 combinaciones.\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   9.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   9.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  10.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  10.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  12.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  13.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  18.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  15.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=   9.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=   9.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=   9.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=   8.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=   8.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=   8.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=   8.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=   8.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=   8.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  13.3s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  11.8s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  11.7s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  12.0s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  11.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  12.0s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  11.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  11.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  11.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  11.3s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  11.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  12.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  11.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  11.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  12.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  16.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  17.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  15.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  14.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  17.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  14.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  14.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  17.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  22.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  15.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  17.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  16.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  16.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  15.6s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  16.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  15.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  15.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  16.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  17.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  17.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  12.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=  15.7s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   9.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  12.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  14.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=  10.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  19.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  11.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=  11.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  17.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  13.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=5; total time=  21.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  15.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  12.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  12.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=  14.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=  14.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=  12.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=5; total time=  15.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=  18.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20; total time=  16.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  26.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  21.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=5; total time=  15.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  15.1s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  14.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=10; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  15.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  15.2s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=  15.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  14.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  15.8s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=5; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  14.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=10; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=  15.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  16.3s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  15.6s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=5; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  14.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  12.9s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=10; total time=  12.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  12.7s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  12.0s\n",
      "[CV] END criterion=entropy, max_depth=20, min_samples_leaf=10, min_samples_split=20; total time=  12.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  14.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  13.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  14.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  14.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=20; total time=  14.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=5; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  13.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  14.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=5; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  13.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10; total time=  13.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  16.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20; total time=  15.6s\n",
      "\n",
      "--- Resultados de GridSearchCV ---\n",
      "Mejor puntuación (f1_macro): 0.9922\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\n",
      "--- Resultados del Modelo OPTIMIZADO en el set de Validación ---\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 98.41%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                      0      1  accuracy     macro avg  weighted avg\n",
      "precision      0.989941    0.0    0.9841      0.494970      0.980041\n",
      "recall         0.994040    0.0    0.9841      0.497020      0.984100\n",
      "f1-score       0.991986    0.0    0.9841      0.495993      0.982066\n",
      "support    19800.000000  200.0    0.9841  20000.000000  20000.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[19682   118]\n",
      " [  200     0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, None],          # Profundidad del árbol. None = sin límite.\n",
    "    'min_samples_leaf': [2, 5, 10],       # Mínimo de muestras en una hoja.\n",
    "    'min_samples_split': [5, 10, 20],     # Mínimo para dividir un nodo.\n",
    "    'criterion': ['gini', 'entropy']      # Criterio para medir la calidad de una división.\n",
    "}\n",
    "\n",
    "# Elegir la métrica de evaluación. 'f1' es una excelente opción de balance.\n",
    "# Para datasets desbalanceados, siempre especificar la clase positiva: 'f1_weighted' o 'roc_auc' son buenas\n",
    "scoring_metric =  'f1_macro'\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "# cv=3 significa 3-fold cross-validation. Para cada una de las 54 combinaciones,\n",
    "# entrenará y validará el modelo 3 veces. Total de entrenamientos: 54 * 3 = 162.\n",
    "grid_search = GridSearchCV(estimator=rf, \n",
    "                            param_grid=param_grid, \n",
    "                            cv=3, \n",
    "                            scoring=scoring_metric,\n",
    "                            verbose=2) # verbose=2 te mostrará el progreso\n",
    "\n",
    "# --- PASO 3: Ejecutar la búsqueda ---\n",
    "print(f\"\\nIniciando GridSearchCV... Probando {len(param_grid['max_depth'])*len(param_grid['min_samples_leaf'])*len(param_grid['min_samples_split'])*len(param_grid['criterion'])} combinaciones.\")\n",
    "# Se entrena sobre los datos remuestreados\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# --- PASO 4: Analizar los resultados ---\n",
    "print(\"\\n--- Resultados de GridSearchCV ---\")\n",
    "print(f\"Mejor puntuación ({scoring_metric}): {grid_search.best_score_:.4f}\")\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# El objeto `grid_search` se re-entrena automáticamente con los mejores parámetros\n",
    "# sobre TODO el conjunto de datos que le pasaste (X_train_resampled).\n",
    "# Por lo tanto, `grid_search.best_estimator_` es tu modelo final y optimizado.\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# --- PASO 5: Evaluar el modelo optimizado en el set de validación ---\n",
    "print(\"\\n--- Resultados del Modelo OPTIMIZADO en el set de Validación ---\")\n",
    "# Usamos el set de validación original, que el modelo nunca ha visto.\n",
    "print_score(best_rf_model, X_train_resampled, y_train_resampled, X_val, y_val, train=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
