{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_score\n",
    "from preprocess import preprocess_fraud_data\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, confusion_matrix, recall_score\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"./data/dataset_balanceado.csv\"\n",
    "df = pd.read_csv(ruta)\n",
    "\n",
    "data = preprocess_fraud_data(df)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Base de Random Forest** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 99.98%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.999833    1.000000  0.999848     0.999917      0.999849\n",
      "recall        1.000000    0.998333  0.999848     0.999167      0.999848\n",
      "f1-score      0.999917    0.999166  0.999848     0.999541      0.999848\n",
      "support    6000.000000  600.000000  0.999848  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[6000    0]\n",
      " [   1  599]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 90.86%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909050    0.0  0.908636     0.454525      0.826409\n",
      "recall        0.999500    0.0  0.908636     0.499750      0.908636\n",
      "f1-score      0.952131    0.0  0.908636     0.476066      0.865574\n",
      "support    2000.000000  200.0  0.908636  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1999    1]\n",
      " [ 200    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo = RandomForestClassifier(random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> El modelo base Random Forest con sus parámetros por defecto, sufre de `sobreajuste` (overfitting), lo que significa que ha vuelto un experto en memorizar los datos de entrenamiento, pero no es capaz de detectar ningún fraude en datos que nunca ha visto, lo cual lo convierte en un modelo inútil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 100 árboles.\n",
      "------------------------------\n",
      "Profundidad máxima encontrada: 33\n",
      "Profundidad mínima encontrada: 21\n",
      "Profundidad promedio: 25.85\n",
      "\n",
      "Profundidades de los primeros 10 árboles:\n",
      "[22, 23, 22, 22, 24, 25, 23, 23, 27, 24]\n"
     ]
    }
   ],
   "source": [
    "#modelo.estimators_ es una lista de todos los árboles de decisión entrenados\n",
    "profundidades = [arbol.get_depth() for arbol in modelo.estimators_]\n",
    "\n",
    "print(f\"El modelo tiene {len(profundidades)} árboles.\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Profundidad máxima encontrada: {max(profundidades)}\")\n",
    "print(f\"Profundidad mínima encontrada: {min(profundidades)}\")\n",
    "print(f\"Profundidad promedio: {np.mean(profundidades):.2f}\")\n",
    "\n",
    "print(\"\\nProfundidades de los primeros 10 árboles:\")\n",
    "print(profundidades[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> El modelo cuenta con 100 árboles, esto árboles tienen una profundidad máxima de 33, la mínima de 21 y el promedio es de casi 26 niveles.\n",
    ">\n",
    "> Al no ponerle límites (max_depth=None) a los árboles, se permitió que cada árbol creciera sin control, lo que los llevó a seguir bajando y hacer preguntas muy específicas permitiendoles ser demasiados detallados, lo cual les permite aprende hasta el mínimo detalle y memorizarlo, por eso es que el modelo funciona tan bien en entrenamiento pero es pésimo en validación, porque son datos que nunca a visto, el modelo solo memoriza no sabe generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podar árboles:** Limitar la profundidad y que cada hoja tenga un número mínimo de ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 90.91%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909091    0.0  0.909091     0.454545      0.826446\n",
      "recall        1.000000    0.0  0.909091     0.500000      0.909091\n",
      "f1-score      0.952381    0.0  0.909091     0.476190      0.865801\n",
      "support    6000.000000  600.0  0.909091  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[6000    0]\n",
      " [ 600    0]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 90.91%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.909091    0.0  0.909091     0.454545      0.826446\n",
      "recall        1.000000    0.0  0.909091     0.500000      0.909091\n",
      "f1-score      0.952381    0.0  0.909091     0.476190      0.865801\n",
      "support    2000.000000  200.0  0.909091  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[2000    0]\n",
      " [ 200    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo_mejorado = RandomForestClassifier(max_depth=10, min_samples_leaf=10, min_samples_split=10, random_state=42)\n",
    "modelo_mejorado.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo_mejorado, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo_mejorado, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Al podar los árboles con max_depth, min_samples_leaf y min_samples_split, se ha resuelto el problema del sobreajuste, sin embargo, al hacerlo, se ha vuelto al problema inicial de nuestros datos, el desbalance de clases. El modelo actual, aunque ya no memoriza, sigue siendo incapaz de identificar el fraude, lo que lo mantiene siendo un modelo inútil.\n",
    ">\n",
    "> Ahora que el modelo no puede memorizar casos específicos (debido a la poda), se ve forzado a aprender reglas generales. Dado que la clase fraude, es la clase minoritaria, la estrategia por la que opta el modelo es ignorar por completo a la clase con menor cantidda e muestras, el algoritmo concluye que el costo de equivocarse al intentar predecir un fraude es mayor que el beneficio de acertar, por lo que adopta la política de siempre predecir la clase mayoritaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class_weight='balanced'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 94.68%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.970672    0.707846  0.946818     0.839259      0.946778\n",
      "recall        0.970833    0.706667  0.946818     0.838750      0.946818\n",
      "f1-score      0.970752    0.707256  0.946818     0.839004      0.946798\n",
      "support    6000.000000  600.000000  0.946818  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[5825  175]\n",
      " [ 176  424]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 86.32%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.908217    0.075630  0.863182     0.491924      0.832527\n",
      "recall        0.945000    0.045000  0.863182     0.495000      0.863182\n",
      "f1-score      0.926244    0.056426  0.863182     0.491335      0.847169\n",
      "support    2000.000000  200.000000  0.863182  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1890  110]\n",
      " [ 191    9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo_balanceado = RandomForestClassifier(max_depth=10, min_samples_leaf=10, min_samples_split=10, class_weight='balanced', random_state=42)\n",
    "modelo_balanceado.fit(X_train, y_train)\n",
    "\n",
    "print_score(modelo_balanceado, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(modelo_balanceado, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Al combinar la regularización para controlar el sobreajuste con el parámetro class_weight='balanced' para atacar el desbalanceo, el modelo ha aprendido a identificar transacciones fraudulentas, se ha pasado de un modelo que ignoraba por completo el fraude (Recall de 0.0) a uno que lo detecta, sin embargo, los resultados actuales generan muchas falsas alarmas, y todavía no es capaz de identificar correctamente todos los verdaderos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ajuste de hiperpárametros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Mejores parámetros encontrados:\n",
      "{'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 15, 'n_estimators': 100}\n",
      "\n",
      "Mejor F1-score durante la validación cruzada:\n",
      "0.10188426898197005\n",
      "\n",
      "--- Reporte de Clasificación en Validación con el Mejor Modelo ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      2000\n",
      "           1       0.12      0.17      0.14       200\n",
      "\n",
      "    accuracy                           0.81      2200\n",
      "   macro avg       0.51      0.52      0.52      2200\n",
      "weighted avg       0.84      0.81      0.82      2200\n",
      "\n",
      "[[1750  250]\n",
      " [ 167   33]]\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],         \n",
    "    'max_depth': [7, 10, 15],           \n",
    "    'min_samples_leaf': [10, 15, 20], \n",
    "    'max_features': ['sqrt', 'log2']    \n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,  \n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nMejor F1-score durante la validación cruzada:\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_val = best_rf_model.predict(X_val)\n",
    "print(\"\\n--- Reporte de Clasificación en Validación con el Mejor Modelo ---\")\n",
    "\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(confusion_matrix(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Mejores parámetros encontrados:\n",
      "{'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 20, 'n_estimators': 100}\n",
      "\n",
      "Mejor F1-score durante la validación cruzada:\n",
      "0.12666666666666665\n",
      "\n",
      "--- Reporte de Clasificación en Validación con el Mejor Modelo ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      2000\n",
      "           1       0.12      0.18      0.14       200\n",
      "\n",
      "    accuracy                           0.80      2200\n",
      "   macro avg       0.52      0.52      0.51      2200\n",
      "weighted avg       0.84      0.80      0.82      2200\n",
      "\n",
      "[[1721  279]\n",
      " [ 163   37]]\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],         \n",
    "    'max_depth': [7, 10, 15],           \n",
    "    'min_samples_leaf': [10, 15, 20], \n",
    "    'max_features': ['sqrt', 'log2']    \n",
    "}\n",
    "\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# cv=5 significa validación cruzada de 5 folds.\n",
    "# n_jobs=-1 usa todos los núcleos de tu CPU para acelerar el proceso.\n",
    "# verbose=2 te dará mensajes de progreso.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=recall_scorer,  #La métrica para decidir qué combinación es la mejor\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nMejor F1-score durante la validación cruzada:\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_val = best_rf_model.predict(X_val)\n",
    "print(\"\\n--- Reporte de Clasificación en Validación con el Mejor Modelo ---\")\n",
    "\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(confusion_matrix(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 84.56%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.927114    0.227568  0.845606     0.577341      0.863519\n",
      "recall        0.901000    0.291667  0.845606     0.596333      0.845606\n",
      "f1-score      0.913870    0.255661  0.845606     0.584766      0.854033\n",
      "support    6000.000000  600.000000  0.845606  6600.000000   6600.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[5406  594]\n",
      " [ 425  175]]\n",
      "\n",
      "Validation Result:\n",
      "================================================\n",
      "Accuracy Score: 81.41%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.907322    0.076923  0.814091     0.492123      0.831831\n",
      "recall        0.886000    0.095000  0.814091     0.490500      0.814091\n",
      "f1-score      0.896534    0.085011  0.814091     0.490773      0.822759\n",
      "support    2000.000000  200.000000  0.814091  2200.000000   2200.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1772  228]\n",
      " [ 181   19]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Pipeline que primero aplica SMOTE y luego entrena el clasificador regularizado\n",
    "pipeline_smote = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(max_depth=10, min_samples_leaf=10, random_state=42))\n",
    "])\n",
    "pipeline_smote.fit(X_train, y_train)\n",
    "\n",
    "print_score(pipeline_smote, X_train, y_train, X_val, y_val, train=True)\n",
    "print_score(pipeline_smote, X_train, y_train, X_val, y_val, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ajustar hiperpárametros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando GridSearchCV con Pipeline de SMOTE...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Mejores parámetros para el Pipeline con SMOTE:\n",
      "{'classifier__max_depth': 8, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 15, 'classifier__n_estimators': 150}\n",
      "\n",
      "Mejor F1-score durante la validación cruzada:\n",
      "0.0932162214437369\n",
      "\n",
      "--- Reporte de Clasificación en Validación con el Pipeline Optimizado ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87      2000\n",
      "           1       0.08      0.14      0.10       200\n",
      "\n",
      "    accuracy                           0.77      2200\n",
      "   macro avg       0.49      0.49      0.48      2200\n",
      "weighted avg       0.83      0.77      0.80      2200\n",
      "\n",
      "[[1661  339]\n",
      " [ 171   29]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# 1. Crear el Pipeline\n",
    "# El clasificador dentro del pipeline NO lleva class_weight\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Definir el Grid de Parámetros (¡justificado!)\n",
    "# Basado en tu diagnóstico (árboles muy profundos), vamos a explorar rangos razonables.\n",
    "# Le decimos a GridSearchCV a qué paso del pipeline pertenecen los parámetros con 'nombre_paso__'\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [150, 250],\n",
    "    'classifier__max_depth': [8, 12, 16],        # Rango controlado, partiendo de tu mejor modelo (10)\n",
    "    'classifier__min_samples_leaf': [10, 15],    # Rango controlado, partiendo de tu mejor modelo (10)\n",
    "    'classifier__max_features': ['sqrt', 0.5]  # 'sqrt' es bueno, 0.5 es otra opción común\n",
    "}\n",
    "\n",
    "# 3. Configurar y Ejecutar GridSearchCV\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid_search_smote = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=5, # 5-fold CV es más robusto que 3\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Iniciando GridSearchCV con Pipeline de SMOTE...\")\n",
    "grid_search_smote.fit(X_train, y_train)\n",
    "\n",
    "# 4. Analizar los Resultados\n",
    "print(\"\\nMejores parámetros para el Pipeline con SMOTE:\")\n",
    "print(grid_search_smote.best_params_)\n",
    "\n",
    "print(\"\\nMejor F1-score durante la validación cruzada:\")\n",
    "print(grid_search_smote.best_score_)\n",
    "\n",
    "# 5. Evaluar el mejor modelo en validación\n",
    "best_pipeline_model = grid_search_smote.best_estimator_\n",
    "y_pred_val = best_pipeline_model.predict(X_val)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación en Validación con el Pipeline Optimizado ---\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(confusion_matrix(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTEENN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultados con SMOTEENN (Combinado) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.59      0.72      2000\n",
      "           1       0.09      0.40      0.14       200\n",
      "\n",
      "    accuracy                           0.57      2200\n",
      "   macro avg       0.50      0.49      0.43      2200\n",
      "weighted avg       0.83      0.57      0.67      2200\n",
      "\n",
      "[[1186  814]\n",
      " [ 121   79]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Define el modelo\n",
    "rf = RandomForestClassifier(max_depth=10, min_samples_leaf=10, random_state=42)\n",
    "\n",
    "# Define la técnica SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "# Crea el pipeline\n",
    "pipeline_smoteenn = ImbPipeline([\n",
    "    ('sampler', smote_enn),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "# Entrena el pipeline\n",
    "pipeline_smoteenn.fit(X_train, y_train)\n",
    "\n",
    "# Evalúa\n",
    "print(\"--- Resultados con SMOTEENN (Combinado) ---\")\n",
    "y_pred_smoteenn = pipeline_smoteenn.predict(X_val)\n",
    "print(classification_report(y_val, y_pred_smoteenn))\n",
    "print(confusion_matrix(y_val, y_pred_smoteenn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Conclusiones:**\n",
    ">\n",
    "> El modelo base demostró sobreajuste, tuvo un rendimiento perfecto en entrenamiento pero nulo en validación (Recall de 0.0), causado por árboles de una profundidad media de 26 niveles.\n",
    "> \n",
    "> Se exploraron dos estrategias principales, la primera consistió en regularizar el modelo (limitando max_depth y min_samples_leaf) y aplicar un ajuste de pesos con class_weight='balanced'. Tras una optimización con GridSearchCV, este enfoque alcanzó un F1-score de 0.14 y un Recall del 17% en el conjunto de validación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
